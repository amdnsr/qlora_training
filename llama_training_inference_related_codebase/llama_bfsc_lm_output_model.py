# -*- coding: utf-8 -*-
"""llama_bfsc_lm_output_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D2bfmX8V7aMc-TPsAHlh0ysvywr-9b6p
"""

!pip install datasets evaluate transformers

# !pip install accelerate

!pip install sentencepiece

!pip install transformers[torch]

import transformers

print(transformers.__version__)

# # Fine-tuning a model on a text classification task

task = "cola"
model_checkpoint = "distilbert-base-uncased"
batch_size = 1

# ## Loading the dataset

# We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.

from datasets import load_dataset, load_metric
import evaluate
# Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell.

# actual_task = "mnli" if task == "mnli-mm" else task
# dataset = load_dataset("glue", actual_task)

import pickle

test_file = 'llama_reddit_100_test.pickle'
# test_file = 'llama_reddit_100_test.pickle'
# test_file = 'llama_solid_extended_100_test.pickle'
# test_file = 'llama_default_reddit_100_test.pickle'
# test_file = 'llama_default_gab_100_test.pickle'
# test_file = 'llama_default_solid_extended_100_test.pickle'

# test_files = ['llama_reddit_100_test.pickle', 'llama_reddit_100_test.pickle']
with open(test_file, 'rb') as handle:
    test = pickle.load(handle)


def mapping_function(example):
    if example['labels'] == "HATEFUL":
        example['labels'] = 1
    else:
        example['labels'] = 0
    return example


import pandas as pd
data = pd.DataFrame({"input": test['predictions'], "labels": test['real_answers']})
from datasets import Dataset
dataset = Dataset.from_pandas(data)
dataset = dataset.map(mapping_function)
trainval_test_dataset = dataset.train_test_split(test_size=int(0.2*len(dataset)), seed=42)


test_dataset = trainval_test_dataset['test']
trainval_dataset = trainval_test_dataset['train']

train_val_dataset = trainval_dataset.train_test_split(test_size=int(0.2*len(trainval_dataset)), seed=42)

dataset = train_val_dataset
# dataset = load_dataset("llama_bfsc_lm_output_data")
# metric = load_metric('glue', actual_task)
metric = evaluate.load("f1")

dataset['train'][0]

import numpy as np

fake_preds = np.random.randint(0, 2, size=(64,))
fake_labels = np.random.randint(0, 2, size=(64,))
metric.compute(predictions=fake_preds, references=fake_labels)


from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

# We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument.

# You can directly call this tokenizer on one sentence or a pair of sentences:

tokenizer("Hello, this one sentence!", "And this sentence goes with it.")

def preprocess_function(example):
    return tokenizer(example['input'], truncation=True)

preprocess_function(dataset['train'][:5])


encoded_dataset = dataset.map(preprocess_function, batched=True)

encoded_dataset['train']['input']

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# num_labels = 3 if task.startswith("mnli") else 1 if task=="stsb" else 2

num_labels = 2

# device_no = 0
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)


# metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
metric_name = "f1"
model_name = model_checkpoint.split("/")[-1]

args = TrainingArguments(
    f"{model_name}-finetuned-{task}",
    evaluation_strategy = "steps",
    save_strategy = "steps",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    logging_steps=10
    # push_to_hub=True,
)


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # if task != "stsb":
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

# Then we just need to pass all of this along with our datasets to the `Trainer`:

# validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"

validation_key = "test"

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

encoded_dataset

# !pip install accelerate==0.20.3

trainer.train()

# We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):

model.save_pretrained(f"{model_name}-finetuned-{task}")

!ls -la distilbert-base-uncased-finetuned-cola/pytorch_model.bin

trainer.train()

# We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):

trainer.evaluate()

from google.colab import drive
drive.mount('/content/drive')

!cp distilbert-base-uncased-finetuned-cola/config.json /content/drive/MyDrive/ml-models/llama_training_reddit_colab_1_ep_128b/

!cp -r distilbert-base-uncased-finetuned-cola/runs /content/drive/MyDrive/ml-models/llama_training_reddit_colab_1_ep_128b/bfsc_model/

# To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).

# You can now upload the result of the training to the Hub, just execute this instruction:

trainer.push_to_hub()

# You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `"your-username/the-name-you-picked"` so for instance:
#
# ```python
# from transformers import AutoModelForSequenceClassification
#
# model = AutoModelForSequenceClassification.from_pretrained("sgugger/my-awesome-model")
# ```

# ## Hyperparameter search

# The `Trainer` supports hyperparameter search using [optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/). For this last section you will need either of those libraries installed, just uncomment the line you want on the next cell and run it.

# ! pip install optuna
# ! pip install ray[tune]

# During hyperparameter search, the `Trainer` will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

# And we can instantiate our `Trainer` like before:

trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# The method we call this time is `hyperparameter_search`. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the `train_dataset` line above by:
# ```python
# train_dataset = encoded_dataset["train"].shard(index=1, num_shards=10)
# ```
# for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search.

best_run = trainer.hyperparameter_search(n_trials=10, direction="maximize")

# The `hyperparameter_search` method returns a `BestRun` objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run.

best_run

# You can customize the objective to maximize by passing along a `compute_objective` function to the `hyperparameter_search` method, and you can customize the search space by passing a `hp_space` argument to `hyperparameter_search`. See this [forum post](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10) for some examples.
#
# To reproduce the best training, just set the hyperparameters in your `TrainingArgument` before creating a `Trainer`:

for n, v in best_run.hyperparameters.items():
    setattr(trainer.args, n, v)

trainer.train()